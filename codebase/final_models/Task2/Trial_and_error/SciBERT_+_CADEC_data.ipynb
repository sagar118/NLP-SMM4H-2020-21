{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SciBERT + CADEC data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 - SciBERT based NER"
      ],
      "metadata": {
        "id": "P7zwxGTiT9h0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install necessary libraries"
      ],
      "metadata": {
        "id": "1CwyeXmxUA8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q contractions transformers sent2vec imbalanced-learn seqeval[gpu]\n",
        "!pip install -q tf-estimator-nightly==2.8.0.dev2021122109\n",
        "!python -m pip uninstall -q -y spacy\n",
        "!python -m pip install -q -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX0IrheMfv-2",
        "outputId": "ca66e297-4049-4f91-ed7e-eeb886a70f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.0 MB 30.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 287 kB 51.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 106 kB 72.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 60.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 880 kB 60.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 61.4 MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 462 kB 35.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 27.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 62.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 653 kB 63.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 457 kB 71.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 80.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_trf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhCetP3jN82Q",
        "outputId": "b710465d-47b0-4482-d732-cd9a6bc6abe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-trf==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.3.0/en_core_web_trf-3.3.0-py3-none-any.whl (460.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 460.3 MB 27 kB/s \n",
            "\u001b[?25hCollecting spacy-transformers<1.2.0,>=1.1.2\n",
            "  Downloading spacy_transformers-1.1.5-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 177 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-trf==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (21.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (8.0.15)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2021.10.8)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.8.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 21.7 MB/s \n",
            "\u001b[?25hCollecting transformers<4.18.0,>=3.4.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (0.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (0.0.53)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (4.11.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-trf==3.3.0) (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.3.0) (1.1.0)\n",
            "Installing collected packages: transformers, spacy-alignments, spacy-transformers, en-core-web-trf\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.18.0\n",
            "    Uninstalling transformers-4.18.0:\n",
            "      Successfully uninstalled transformers-4.18.0\n",
            "Successfully installed en-core-web-trf-3.3.0 spacy-alignments-0.8.5 spacy-transformers-1.1.5 transformers-4.17.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_trf')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "ig69Wo23JN05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load all libraries"
      ],
      "metadata": {
        "id": "w68FFL6Njvfh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDgpo4Und_U4",
        "outputId": "74a1673b-7e39-4d12-a32c-ddd2bdd64979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import warnings\n",
        "import torch.nn as nn\n",
        "\n",
        "from spacy.util import minibatch, compounding\n",
        "from spacy.training import Example\n",
        "from pathlib import Path\n",
        "from spacy.training import offsets_to_biluo_tags\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pprint import pprint\n",
        "# from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "pd.options.display.max_rows = None\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_colwidth=None\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load the dataset"
      ],
      "metadata": {
        "id": "dGOCNgOwUGJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "training = pd.read_csv('training_data_with_ADR.csv')\n",
        "validation = pd.read_csv('validation_data_with_ADR.csv')"
      ],
      "metadata": {
        "id": "KETa291bfc0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cadec_data = pd.read_excel('cadec_data.xlsx')"
      ],
      "metadata": {
        "id": "jRDRsuRDC0qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(training.shape)\n",
        "print(validation.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvRfUpX5_QkJ",
        "outputId": "7d57c6cf-4dc9-4526-ed6b-307e84c35ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2172, 13)\n",
            "(560, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cadec_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBrtoOdODuf2",
        "outputId": "36a0298c-5a59-419e-e371-b244d682bf5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6315, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DZk3-6d9HM9O",
        "outputId": "0c9eab3a-e9d7-4e77-91f0-8cfb87059127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0            tweet_id  begin  end type extraction         drug  \\\n",
              "0           0  331187619096588288    NaN  NaN  NaN        NaN    ofloxacin   \n",
              "1           1  332227554956161024    NaN  NaN  NaN        NaN    trazodone   \n",
              "2           2  332448217490944000    NaN  NaN  NaN        NaN  lamotrigine   \n",
              "3           3  332977955754110976    NaN  NaN  NaN        NaN     cymbalta   \n",
              "4           4  333674203331051520    NaN  NaN  NaN        NaN     seroquel   \n",
              "\n",
              "                                                                                                                                         tweet  \\\n",
              "0  @seefisch:oral drugs for pyelonephritis:ciprofloxacin levofloxacin tmp/smz do not use nitrofurantoin for pyelo(only cystitis)@david_medinaf   \n",
              "1       happy for wellbutrin; has similar effects as adderall.. trazodone is super promising for sleep.. but abilify can cause weight gain -_-   \n",
              "2  @stilgarg i'm ok ty have an official diagnosis of bipolar now, feeling ok at the moment lamotrigine has been increased having monotherapy:/   \n",
              "3                                                                                                i'm soo depressed cymbalta couldn't help me .   \n",
              "4                        time for my daily afternoon relaxation ritual of smoking weed, taking 2 mgs of clonazepam, and 400 mg of seroquel xr.   \n",
              "\n",
              "   meddra_code meddra_term  \\\n",
              "0          NaN         NaN   \n",
              "1          NaN         NaN   \n",
              "2          NaN         NaN   \n",
              "3          NaN         NaN   \n",
              "4          NaN         NaN   \n",
              "\n",
              "                                                                                                                                                    clean_tweets  \\\n",
              "0                     <user> : oral drugs for pyelonephritis : ciprofloxacin levofloxacin tmp / smz do not use nitrofurantoin for pyelo ( only cystitis ) <user>   \n",
              "1  happy for wellbutrin ; has similar effects as adderall . <repeated> trazodone is super promising for sleep . <repeated> but abilify can cause weight gain -_-   \n",
              "2             <user> i am ok ty have an official diagnosis of bipolar now , feeling ok at the moment lamotrigine has been increased having monotherapy <annoyed>   \n",
              "3                                                                                                                i am soo depressed cymbalta could not help me .   \n",
              "4                           time for my daily afternoon relaxation ritual of smoking weed , taking <number> mgs of clonazepam , and <number> mg of seroquel xr .   \n",
              "\n",
              "   label  ADR  \n",
              "0      0    0  \n",
              "1      0    0  \n",
              "2      0    0  \n",
              "3      0    0  \n",
              "4      0    0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-064de781-9c3f-4225-af8e-5b5291a10c1a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>begin</th>\n",
              "      <th>end</th>\n",
              "      <th>type</th>\n",
              "      <th>extraction</th>\n",
              "      <th>drug</th>\n",
              "      <th>tweet</th>\n",
              "      <th>meddra_code</th>\n",
              "      <th>meddra_term</th>\n",
              "      <th>clean_tweets</th>\n",
              "      <th>label</th>\n",
              "      <th>ADR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>331187619096588288</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ofloxacin</td>\n",
              "      <td>@seefisch:oral drugs for pyelonephritis:ciprofloxacin levofloxacin tmp/smz do not use nitrofurantoin for pyelo(only cystitis)@david_medinaf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;user&gt; : oral drugs for pyelonephritis : ciprofloxacin levofloxacin tmp / smz do not use nitrofurantoin for pyelo ( only cystitis ) &lt;user&gt;</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>332227554956161024</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>trazodone</td>\n",
              "      <td>happy for wellbutrin; has similar effects as adderall.. trazodone is super promising for sleep.. but abilify can cause weight gain -_-</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>happy for wellbutrin ; has similar effects as adderall . &lt;repeated&gt; trazodone is super promising for sleep . &lt;repeated&gt; but abilify can cause weight gain -_-</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>332448217490944000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>lamotrigine</td>\n",
              "      <td>@stilgarg i'm ok ty have an official diagnosis of bipolar now, feeling ok at the moment lamotrigine has been increased having monotherapy:/</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;user&gt; i am ok ty have an official diagnosis of bipolar now , feeling ok at the moment lamotrigine has been increased having monotherapy &lt;annoyed&gt;</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>332977955754110976</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cymbalta</td>\n",
              "      <td>i'm soo depressed cymbalta couldn't help me .</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>i am soo depressed cymbalta could not help me .</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>333674203331051520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>seroquel</td>\n",
              "      <td>time for my daily afternoon relaxation ritual of smoking weed, taking 2 mgs of clonazepam, and 400 mg of seroquel xr.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>time for my daily afternoon relaxation ritual of smoking weed , taking &lt;number&gt; mgs of clonazepam , and &lt;number&gt; mg of seroquel xr .</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-064de781-9c3f-4225-af8e-5b5291a10c1a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-064de781-9c3f-4225-af8e-5b5291a10c1a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-064de781-9c3f-4225-af8e-5b5291a10c1a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cadec_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6so03TqZDyv3",
        "outputId": "e7c944ac-7f52-41be-c321-f51db5b312d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  \\\n",
              "0           0   \n",
              "1           1   \n",
              "2           2   \n",
              "3           3   \n",
              "4           4   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
              "0                                                  I am left with chest pain, muscle weakness and spasms and I am up every 3 hrs drinking water. Drs. laugh when I say that I have Rhabdomyolosis - my Cpk or ck or whatever it is is fine. but no one cks the myoglobin or hemoglogin in urine. I'm desparate for a Dr in Mn. that will do that. I don't know where to turn and get sicker and sicker. Am afraid my heart will go. The drug gets cholestorel down, but ruins your life.   \n",
              "1                                                  I am left with chest pain, muscle weakness and spasms and I am up every 3 hrs drinking water. Drs. laugh when I say that I have Rhabdomyolosis - my Cpk or ck or whatever it is is fine. but no one cks the myoglobin or hemoglogin in urine. I'm desparate for a Dr in Mn. that will do that. I don't know where to turn and get sicker and sicker. Am afraid my heart will go. The drug gets cholestorel down, but ruins your life.   \n",
              "2                                                  I am left with chest pain, muscle weakness and spasms and I am up every 3 hrs drinking water. Drs. laugh when I say that I have Rhabdomyolosis - my Cpk or ck or whatever it is is fine. but no one cks the myoglobin or hemoglogin in urine. I'm desparate for a Dr in Mn. that will do that. I don't know where to turn and get sicker and sicker. Am afraid my heart will go. The drug gets cholestorel down, but ruins your life.   \n",
              "3                                                  I am left with chest pain, muscle weakness and spasms and I am up every 3 hrs drinking water. Drs. laugh when I say that I have Rhabdomyolosis - my Cpk or ck or whatever it is is fine. but no one cks the myoglobin or hemoglogin in urine. I'm desparate for a Dr in Mn. that will do that. I don't know where to turn and get sicker and sicker. Am afraid my heart will go. The drug gets cholestorel down, but ruins your life.   \n",
              "4  within 2 hours started having heart attack symptoms. Had to go to ER. I was in ER within 3 hours of taking 1st pill. It was scary trying to make sure I had proper observation and not over-reaction because of the symptoms. I had to slow people down several times. Now convinced it would have had serious health risks for me had I even tried taking a second pill the next day or even continuing taking them. It took a while (months) before I started to feel normal again.   \n",
              "\n",
              "                   extraction    indexes  \\\n",
              "0                  chest pain  ['15 25']   \n",
              "1             muscle weakness  ['27 42']   \n",
              "2                      spasms  ['47 53']   \n",
              "3  every 3 hrs drinking water  ['66 92']   \n",
              "4       heart attack symptoms  ['30 51']   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             text_tokens  \\\n",
              "0                                                 [I, am, left, with, chest, pain, ,, muscle, weakness, and, spasms, and, I, am, up, every, 3, hrs, drinking, water, ., Drs, ., laugh, when, I, say, that, I, have, Rhabdomyolosis, -, my, Cpk, or, ck, or, whatever, it, is, is, fine, ., but, no, one, cks, the, myoglobin, or, hemoglogin, in, urine, ., I, 'm, desparate, for, a, Dr, in, Mn, ., that, will, do, that, ., I, do, n't, know, where, to, turn, and, get, sicker, and, sicker, ., Am, afraid, my, heart, will, go, ., The, drug, gets, cholestorel, down, ,, but, ruins, your, life, .]   \n",
              "1                                                 [I, am, left, with, chest, pain, ,, muscle, weakness, and, spasms, and, I, am, up, every, 3, hrs, drinking, water, ., Drs, ., laugh, when, I, say, that, I, have, Rhabdomyolosis, -, my, Cpk, or, ck, or, whatever, it, is, is, fine, ., but, no, one, cks, the, myoglobin, or, hemoglogin, in, urine, ., I, 'm, desparate, for, a, Dr, in, Mn, ., that, will, do, that, ., I, do, n't, know, where, to, turn, and, get, sicker, and, sicker, ., Am, afraid, my, heart, will, go, ., The, drug, gets, cholestorel, down, ,, but, ruins, your, life, .]   \n",
              "2                                                 [I, am, left, with, chest, pain, ,, muscle, weakness, and, spasms, and, I, am, up, every, 3, hrs, drinking, water, ., Drs, ., laugh, when, I, say, that, I, have, Rhabdomyolosis, -, my, Cpk, or, ck, or, whatever, it, is, is, fine, ., but, no, one, cks, the, myoglobin, or, hemoglogin, in, urine, ., I, 'm, desparate, for, a, Dr, in, Mn, ., that, will, do, that, ., I, do, n't, know, where, to, turn, and, get, sicker, and, sicker, ., Am, afraid, my, heart, will, go, ., The, drug, gets, cholestorel, down, ,, but, ruins, your, life, .]   \n",
              "3                                                 [I, am, left, with, chest, pain, ,, muscle, weakness, and, spasms, and, I, am, up, every, 3, hrs, drinking, water, ., Drs, ., laugh, when, I, say, that, I, have, Rhabdomyolosis, -, my, Cpk, or, ck, or, whatever, it, is, is, fine, ., but, no, one, cks, the, myoglobin, or, hemoglogin, in, urine, ., I, 'm, desparate, for, a, Dr, in, Mn, ., that, will, do, that, ., I, do, n't, know, where, to, turn, and, get, sicker, and, sicker, ., Am, afraid, my, heart, will, go, ., The, drug, gets, cholestorel, down, ,, but, ruins, your, life, .]   \n",
              "4  [within, 2, hours, started, having, heart, attack, symptoms, ., Had, to, go, to, ER, ., I, was, in, ER, within, 3, hours, of, taking, 1st, pill, ., It, was, scary, trying, to, make, sure, I, had, proper, observation, and, not, over, -, reaction, because, of, the, symptoms, ., I, had, to, slow, people, down, several, times, ., Now, convinced, it, would, have, had, serious, health, risks, for, me, had, I, even, tried, taking, a, second, pill, the, next, day, or, even, continuing, taking, them, ., It, took, a, while, (, months, ), before, I, started, to, feel, normal, again, .]   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              bio_tags  \n",
              "0              ['O', 'O', 'O', 'O', 'B-ADR', 'I-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']  \n",
              "1              ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADR', 'I-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']  \n",
              "2                  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']  \n",
              "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADR', 'I-ADR', 'I-ADR', 'I-ADR', 'I-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']  \n",
              "4     ['O', 'O', 'O', 'O', 'O', 'B-ADR', 'I-ADR', 'I-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72e20713-0020-4e44-81e3-a2f2ce97eed3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>extraction</th>\n",
              "      <th>indexes</th>\n",
              "      <th>text_tokens</th>\n",
              "      <th>bio_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>I am left with chest pain, muscle weakness and spasms and I am up every 3 hrs drinking water. Drs. laugh when I say that I have Rhabdomyolosis - my Cpk or ck or whatever it is is fine. but no one cks the myoglobin or hemoglogin in urine. I'm desparate for a Dr in Mn. that will do that. I don't know where to turn and get sicker and sicker. Am afraid my heart will go. The drug gets cholestorel down, but ruins your life.</td>\n",
              "      <td>chest pain</td>\n",
              "      <td>['15 25']</td>\n",
              "      <td>[I, am, left, with, chest, pain, ,, muscle, weakness, and, spasms, and, I, am, up, every, 3, hrs, drinking, water, ., Drs, ., laugh, when, I, say, that, I, have, Rhabdomyolosis, -, my, Cpk, or, ck, or, whatever, it, is, is, fine, ., but, no, one, cks, the, myoglobin, or, hemoglogin, in, urine, ., I, 'm, desparate, for, a, Dr, in, Mn, ., that, will, do, that, ., I, do, n't, know, where, to, turn, and, get, sicker, and, sicker, ., Am, afraid, my, heart, will, go, ., The, drug, gets, cholestorel, down, ,, but, ruins, your, life, .]</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'B-ADR', 'I-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I am left with chest pain, muscle weakness and spasms and I am up every 3 hrs drinking water. Drs. laugh when I say that I have Rhabdomyolosis - my Cpk or ck or whatever it is is fine. but no one cks the myoglobin or hemoglogin in urine. I'm desparate for a Dr in Mn. that will do that. I don't know where to turn and get sicker and sicker. Am afraid my heart will go. The drug gets cholestorel down, but ruins your life.</td>\n",
              "      <td>muscle weakness</td>\n",
              "      <td>['27 42']</td>\n",
              "      <td>[I, am, left, with, chest, pain, ,, muscle, weakness, and, spasms, and, I, am, up, every, 3, hrs, drinking, water, ., Drs, ., laugh, when, I, say, that, I, have, Rhabdomyolosis, -, my, Cpk, or, ck, or, whatever, it, is, is, fine, ., but, no, one, cks, the, myoglobin, or, hemoglogin, in, urine, ., I, 'm, desparate, for, a, Dr, in, Mn, ., that, will, do, that, ., I, do, n't, know, where, to, turn, and, get, sicker, and, sicker, ., Am, afraid, my, heart, will, go, ., The, drug, gets, cholestorel, down, ,, but, ruins, your, life, .]</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADR', 'I-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I am left with chest pain, muscle weakness and spasms and I am up every 3 hrs drinking water. Drs. laugh when I say that I have Rhabdomyolosis - my Cpk or ck or whatever it is is fine. but no one cks the myoglobin or hemoglogin in urine. I'm desparate for a Dr in Mn. that will do that. I don't know where to turn and get sicker and sicker. Am afraid my heart will go. The drug gets cholestorel down, but ruins your life.</td>\n",
              "      <td>spasms</td>\n",
              "      <td>['47 53']</td>\n",
              "      <td>[I, am, left, with, chest, pain, ,, muscle, weakness, and, spasms, and, I, am, up, every, 3, hrs, drinking, water, ., Drs, ., laugh, when, I, say, that, I, have, Rhabdomyolosis, -, my, Cpk, or, ck, or, whatever, it, is, is, fine, ., but, no, one, cks, the, myoglobin, or, hemoglogin, in, urine, ., I, 'm, desparate, for, a, Dr, in, Mn, ., that, will, do, that, ., I, do, n't, know, where, to, turn, and, get, sicker, and, sicker, ., Am, afraid, my, heart, will, go, ., The, drug, gets, cholestorel, down, ,, but, ruins, your, life, .]</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I am left with chest pain, muscle weakness and spasms and I am up every 3 hrs drinking water. Drs. laugh when I say that I have Rhabdomyolosis - my Cpk or ck or whatever it is is fine. but no one cks the myoglobin or hemoglogin in urine. I'm desparate for a Dr in Mn. that will do that. I don't know where to turn and get sicker and sicker. Am afraid my heart will go. The drug gets cholestorel down, but ruins your life.</td>\n",
              "      <td>every 3 hrs drinking water</td>\n",
              "      <td>['66 92']</td>\n",
              "      <td>[I, am, left, with, chest, pain, ,, muscle, weakness, and, spasms, and, I, am, up, every, 3, hrs, drinking, water, ., Drs, ., laugh, when, I, say, that, I, have, Rhabdomyolosis, -, my, Cpk, or, ck, or, whatever, it, is, is, fine, ., but, no, one, cks, the, myoglobin, or, hemoglogin, in, urine, ., I, 'm, desparate, for, a, Dr, in, Mn, ., that, will, do, that, ., I, do, n't, know, where, to, turn, and, get, sicker, and, sicker, ., Am, afraid, my, heart, will, go, ., The, drug, gets, cholestorel, down, ,, but, ruins, your, life, .]</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADR', 'I-ADR', 'I-ADR', 'I-ADR', 'I-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>within 2 hours started having heart attack symptoms. Had to go to ER. I was in ER within 3 hours of taking 1st pill. It was scary trying to make sure I had proper observation and not over-reaction because of the symptoms. I had to slow people down several times. Now convinced it would have had serious health risks for me had I even tried taking a second pill the next day or even continuing taking them. It took a while (months) before I started to feel normal again.</td>\n",
              "      <td>heart attack symptoms</td>\n",
              "      <td>['30 51']</td>\n",
              "      <td>[within, 2, hours, started, having, heart, attack, symptoms, ., Had, to, go, to, ER, ., I, was, in, ER, within, 3, hours, of, taking, 1st, pill, ., It, was, scary, trying, to, make, sure, I, had, proper, observation, and, not, over, -, reaction, because, of, the, symptoms, ., I, had, to, slow, people, down, several, times, ., Now, convinced, it, would, have, had, serious, health, risks, for, me, had, I, even, tried, taking, a, second, pill, the, next, day, or, even, continuing, taking, them, ., It, took, a, while, (, months, ), before, I, started, to, feel, normal, again, .]</td>\n",
              "      <td>['O', 'O', 'O', 'O', 'O', 'B-ADR', 'I-ADR', 'I-ADR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72e20713-0020-4e44-81e3-a2f2ce97eed3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72e20713-0020-4e44-81e3-a2f2ce97eed3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72e20713-0020-4e44-81e3-a2f2ce97eed3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Prepare the data"
      ],
      "metadata": {
        "id": "6KXsvxW7F8MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the non-null rows\n",
        "training_data = training[training.ADR == 1]\n",
        "validation_data = validation[validation.ADR == 1]\n",
        "\n",
        "# Reset index\n",
        "training_data.reset_index(inplace=True, drop=True)\n",
        "validation_data.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# Drop unwanted column\n",
        "training_data.drop(\"Unnamed: 0\", inplace=True, axis=1)\n",
        "validation_data.drop(\"Unnamed: 0\", inplace=True, axis=1)\n",
        "\n",
        "# Fill in the missing values\n",
        "training_data.extraction = training_data.extraction.fillna('-')\n",
        "validation_data.extraction = validation_data.extraction.fillna('-')"
      ],
      "metadata": {
        "id": "HxLeHeVqH3UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of training data: {training_data.shape}\")\n",
        "print(f\"Shape of validation data: {validation_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132e67cd-1b38-4369-b389-654f65a0ad27",
        "id": "Z6NIsblMH3UB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training data: (1434, 12)\n",
            "Shape of validation data: (391, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ground truth labels were misleading \n",
        "# Hence create a new ground truth for start and end index\n",
        "\n",
        "def find_start_end(dataframe):\n",
        "    count = 0\n",
        "    new_start = list()\n",
        "    new_end = list()\n",
        "\n",
        "    for row in dataframe.itertuples():\n",
        "        if row[5] != '-': # If extraction is not empty then\n",
        "            match = re.search(r\"{}\".format(row[5].lower().replace(\")\", \"\\)\")), r\"{}\".format(row[7].lower()))\n",
        "            if not match:\n",
        "                new_start.append(int(row[2]))\n",
        "                new_end.append(int(row[3]))\n",
        "            else:\n",
        "                if row[2] != match.start() and row[3] != match.end():\n",
        "                    count += 1\n",
        "                new_start.append(match.start())\n",
        "                new_end.append(match.end())\n",
        "        \n",
        "        else: # If extraction is empty then\n",
        "            new_start.append(0)\n",
        "            new_end.append(0)\n",
        "\n",
        "    print(f\"Percentage of rows for which start and end index did not match is {round(count/dataframe.shape[0]*100, 4)}%\")\n",
        "    return new_start, new_end"
      ],
      "metadata": {
        "id": "LC0yoUznHcla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_start, train_end = find_start_end(training_data)\n",
        "valid_start, valid_end = find_start_end(validation_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3YvXREtIbq9",
        "outputId": "de0c5f7e-79c1-4e5d-803b-f45686b88956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of rows for which start and end index did not match is 36.4017%\n",
            "Percentage of rows for which start and end index did not match is 28.9003%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two columns for new start and end index\n",
        "training_data['new_start'] = train_start\n",
        "training_data['new_end'] = train_end\n",
        "\n",
        "validation_data['new_start'] = valid_start\n",
        "validation_data['new_end'] = valid_end"
      ],
      "metadata": {
        "id": "UloFb7eeKXQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tHtg8qQVIqx_",
        "outputId": "87e016fc-de96-49ad-efc9-960bb221703e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             tweet_id  begin    end type   extraction      drug  \\\n",
              "0  342314998904786945   42.0   53.0  ADR  lost vision  cymbalta   \n",
              "1  342314998904786945   11.0   21.0  ADR   withdrawal  cymbalta   \n",
              "2  342322703556038657   27.0   35.0  ADR     nauseous     cipro   \n",
              "3  342349802601844737  109.0  118.0  ADR    can't cum  seroquel   \n",
              "4  342349802601844737  101.0  104.0  ADR          fat  seroquel   \n",
              "\n",
              "                                                                                                                                    tweet  \\\n",
              "0  #cymbalta withdrawal has reached a peak, lost vision and almost crashed my car from a brain zap. thanks a zillion #elililly #bigpharma   \n",
              "1  #cymbalta withdrawal has reached a peak, lost vision and almost crashed my car from a brain zap. thanks a zillion #elililly #bigpharma   \n",
              "2                                                                   i hate cipro! #antibiotic #nauseous #cf #hospitallife #cysticfibrosis   \n",
              "3            @luckystubbs reppin zoloft&amp;seroquel since last november. i'm hella gainin weight too awesome i'm fat and can't cum i own   \n",
              "4            @luckystubbs reppin zoloft&amp;seroquel since last november. i'm hella gainin weight too awesome i'm fat and can't cum i own   \n",
              "\n",
              "   meddra_code          meddra_term  \\\n",
              "0   10047522.0          vision loss   \n",
              "1   10048010.0  withdrawal syndrome   \n",
              "2   10028823.0             nauseous   \n",
              "3   10021574.0  inability to orgasm   \n",
              "4   10047896.0          weight gain   \n",
              "\n",
              "                                                                                                                                                                                             clean_tweets  \\\n",
              "0  <hashtag> cymbalta </hashtag> withdrawal has reached a peak , lost vision and almost crashed my car from a brain zap . thanks a zillion <hashtag> eli lilly </hashtag> <hashtag> big pharma </hashtag>   \n",
              "1  <hashtag> cymbalta </hashtag> withdrawal has reached a peak , lost vision and almost crashed my car from a brain zap . thanks a zillion <hashtag> eli lilly </hashtag> <hashtag> big pharma </hashtag>   \n",
              "2                            i hate cipro ! <hashtag> antibiotic </hashtag> <hashtag> nauseous </hashtag> <hashtag> cf </hashtag> <hashtag> hospital life </hashtag> <hashtag> cystic fibrosis </hashtag>   \n",
              "3                                                                               <user> reppin zoloft & seroquel since last november . i am hella gainin weight too awesome i am fat and can not cum i own   \n",
              "4                                                                               <user> reppin zoloft & seroquel since last november . i am hella gainin weight too awesome i am fat and can not cum i own   \n",
              "\n",
              "   label  ADR  new_start  new_end  \n",
              "0      1    1         41       52  \n",
              "1      1    1         10       20  \n",
              "2      1    1         27       35  \n",
              "3      1    1        109      118  \n",
              "4      1    1        101      104  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7f319367-e322-4a0d-96eb-ec3bae5e7042\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>begin</th>\n",
              "      <th>end</th>\n",
              "      <th>type</th>\n",
              "      <th>extraction</th>\n",
              "      <th>drug</th>\n",
              "      <th>tweet</th>\n",
              "      <th>meddra_code</th>\n",
              "      <th>meddra_term</th>\n",
              "      <th>clean_tweets</th>\n",
              "      <th>label</th>\n",
              "      <th>ADR</th>\n",
              "      <th>new_start</th>\n",
              "      <th>new_end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>342314998904786945</td>\n",
              "      <td>42.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>ADR</td>\n",
              "      <td>lost vision</td>\n",
              "      <td>cymbalta</td>\n",
              "      <td>#cymbalta withdrawal has reached a peak, lost vision and almost crashed my car from a brain zap. thanks a zillion #elililly #bigpharma</td>\n",
              "      <td>10047522.0</td>\n",
              "      <td>vision loss</td>\n",
              "      <td>&lt;hashtag&gt; cymbalta &lt;/hashtag&gt; withdrawal has reached a peak , lost vision and almost crashed my car from a brain zap . thanks a zillion &lt;hashtag&gt; eli lilly &lt;/hashtag&gt; &lt;hashtag&gt; big pharma &lt;/hashtag&gt;</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>41</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>342314998904786945</td>\n",
              "      <td>11.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>ADR</td>\n",
              "      <td>withdrawal</td>\n",
              "      <td>cymbalta</td>\n",
              "      <td>#cymbalta withdrawal has reached a peak, lost vision and almost crashed my car from a brain zap. thanks a zillion #elililly #bigpharma</td>\n",
              "      <td>10048010.0</td>\n",
              "      <td>withdrawal syndrome</td>\n",
              "      <td>&lt;hashtag&gt; cymbalta &lt;/hashtag&gt; withdrawal has reached a peak , lost vision and almost crashed my car from a brain zap . thanks a zillion &lt;hashtag&gt; eli lilly &lt;/hashtag&gt; &lt;hashtag&gt; big pharma &lt;/hashtag&gt;</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>342322703556038657</td>\n",
              "      <td>27.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>ADR</td>\n",
              "      <td>nauseous</td>\n",
              "      <td>cipro</td>\n",
              "      <td>i hate cipro! #antibiotic #nauseous #cf #hospitallife #cysticfibrosis</td>\n",
              "      <td>10028823.0</td>\n",
              "      <td>nauseous</td>\n",
              "      <td>i hate cipro ! &lt;hashtag&gt; antibiotic &lt;/hashtag&gt; &lt;hashtag&gt; nauseous &lt;/hashtag&gt; &lt;hashtag&gt; cf &lt;/hashtag&gt; &lt;hashtag&gt; hospital life &lt;/hashtag&gt; &lt;hashtag&gt; cystic fibrosis &lt;/hashtag&gt;</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>27</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>342349802601844737</td>\n",
              "      <td>109.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>ADR</td>\n",
              "      <td>can't cum</td>\n",
              "      <td>seroquel</td>\n",
              "      <td>@luckystubbs reppin zoloft&amp;amp;seroquel since last november. i'm hella gainin weight too awesome i'm fat and can't cum i own</td>\n",
              "      <td>10021574.0</td>\n",
              "      <td>inability to orgasm</td>\n",
              "      <td>&lt;user&gt; reppin zoloft &amp; seroquel since last november . i am hella gainin weight too awesome i am fat and can not cum i own</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>109</td>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>342349802601844737</td>\n",
              "      <td>101.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>ADR</td>\n",
              "      <td>fat</td>\n",
              "      <td>seroquel</td>\n",
              "      <td>@luckystubbs reppin zoloft&amp;amp;seroquel since last november. i'm hella gainin weight too awesome i'm fat and can't cum i own</td>\n",
              "      <td>10047896.0</td>\n",
              "      <td>weight gain</td>\n",
              "      <td>&lt;user&gt; reppin zoloft &amp; seroquel since last november . i am hella gainin weight too awesome i am fat and can not cum i own</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>101</td>\n",
              "      <td>104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7f319367-e322-4a0d-96eb-ec3bae5e7042')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7f319367-e322-4a0d-96eb-ec3bae5e7042 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7f319367-e322-4a0d-96eb-ec3bae5e7042');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spacy and pipeline\n",
        "nlp = spacy.load('en_core_web_trf')\n",
        "ner = nlp.get_pipe('ner')"
      ],
      "metadata": {
        "id": "8J281OrZF-o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "id": "oQHI6J0TK8di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9eeb5cc-a2bd-4bc0-8860-9fe5c16ac960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the tweets in spacy format\n",
        "TRAIN_DATA = []\n",
        "VALID_DATA = []\n",
        "\n",
        "for row in training_data.itertuples():\n",
        "    TRAIN_DATA.append((row[7],{\n",
        "        'entities': [(int(row[13]), int(row[14]), 'ADR')]\n",
        "    }))\n",
        "\n",
        "for row in validation_data.itertuples():\n",
        "    VALID_DATA.append((row[7],{\n",
        "        'entities': [(int(row[13]), int(row[14]), 'ADR')]\n",
        "    }))"
      ],
      "metadata": {
        "id": "sk3glf5QIxmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tag text to BILUO format and replace the tags \"L\" with \"I\" and \"U\" with \"B\"\n",
        "# for BIO labelling scheme\n",
        "\n",
        "# tags_list = list()\n",
        "# for text, annot in TRAIN_DATA:\n",
        "#     doc = nlp(text)\n",
        "#     tags = offsets_to_biluo_tags(doc, annot['entities'])\n",
        "#     bio_tags = list(map(lambda tag: tag.replace(\"L\", \"I\").replace(\"U\", \"B\"), tags))\n",
        "#     bio_tags = ['O' if tag == '-' else tag for tag in bio_tags]\n",
        "#     tags_list.append(bio_tags)\n",
        "\n",
        "valid_tags_list = list()\n",
        "for text, annot in VALID_DATA:\n",
        "    doc = nlp(text)\n",
        "    tags = offsets_to_biluo_tags(doc, annot['entities'])\n",
        "    bio_tags = list(map(lambda tag: tag.replace(\"L\", \"I\").replace(\"U\", \"B\"), tags))\n",
        "    bio_tags = ['O' if tag == '-' else tag for tag in bio_tags]\n",
        "    valid_tags_list.append(bio_tags)"
      ],
      "metadata": {
        "id": "Bwcp-Sa4GZov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_train = pd.DataFrame(columns=[\"extraction\", \"tweet\", \"bio_tags\"])\n",
        "final_valid = pd.DataFrame(columns=[\"extraction\", \"tweet\", \"bio_tags\"])"
      ],
      "metadata": {
        "id": "-PXY6ZViGy28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_train.extraction = training_data.extraction\n",
        "# final_train.bio_tags = tags_list\n",
        "\n",
        "final_valid.extraction = validation_data.extraction\n",
        "final_valid.bio_tags = valid_tags_list"
      ],
      "metadata": {
        "id": "Yd5FLI96G3Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# final_train.tweet = [[str(token) for token in nlp(tweet)] for tweet in training_data.tweet]\n",
        "final_valid.tweet = [[str(token) for token in nlp(tweet)] for tweet in validation_data.tweet]"
      ],
      "metadata": {
        "id": "nylwomxuJHUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_valid.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "9K2mubBSJgMx",
        "outputId": "3601d86b-6562-4e0d-a6ca-1dc02596b039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      extraction  \\\n",
              "0             AD   \n",
              "1          focus   \n",
              "2           died   \n",
              "3           died   \n",
              "4  tendon damage   \n",
              "\n",
              "                                                                                                                                                                                              tweet  \\\n",
              "0                                                [apparently, ,, baclofen, greatly, exacerbates, the, \", ad, \", part, of, my, adhd, ., average, length, of, focus, today, :, about, 30, seconds, .]   \n",
              "1                                                [apparently, ,, baclofen, greatly, exacerbates, the, \", ad, \", part, of, my, adhd, ., average, length, of, focus, today, :, about, 30, seconds, .]   \n",
              "2                             [pt, of, mine, died, from, cipro, rt, @ciproispoison, :, @gastromom, if, only, more, doctors, thought, like, you, !, i, lost, my, entire, life, to, 12, cipro, pills]   \n",
              "3                      [@gastromom, the, only, pt, of, mine, who, ever, died, was, one, age, 21, profound, autism, chronic, underwt, &, amp, ;, gi, issues, ,, given, lots, of, cipro, ., terrible]   \n",
              "4  [owww, ., i, hurt, my, foot, ., &, amp, ;, am, concerned, ., 1st, warning, on, cipro, is, tendon, damage, ., :-/, it, certainly, was, n't, a, rupture, ., i, 'm, sure, i, 'm, just, paranoid, .]   \n",
              "\n",
              "                                                                                                                  bio_tags  \n",
              "0                                             [O, O, O, O, O, O, O, B-ADR, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
              "1                                             [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ADR, O, O, O, O, O, O]  \n",
              "2                                    [O, O, O, B-ADR, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
              "3                              [O, O, O, O, O, O, O, O, B-ADR, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
              "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ADR, I-ADR, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67e8f3b9-80fd-4a49-9e5a-8c54a4f0c7a4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>extraction</th>\n",
              "      <th>tweet</th>\n",
              "      <th>bio_tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AD</td>\n",
              "      <td>[apparently, ,, baclofen, greatly, exacerbates, the, \", ad, \", part, of, my, adhd, ., average, length, of, focus, today, :, about, 30, seconds, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, B-ADR, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>focus</td>\n",
              "      <td>[apparently, ,, baclofen, greatly, exacerbates, the, \", ad, \", part, of, my, adhd, ., average, length, of, focus, today, :, about, 30, seconds, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ADR, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>died</td>\n",
              "      <td>[pt, of, mine, died, from, cipro, rt, @ciproispoison, :, @gastromom, if, only, more, doctors, thought, like, you, !, i, lost, my, entire, life, to, 12, cipro, pills]</td>\n",
              "      <td>[O, O, O, B-ADR, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>died</td>\n",
              "      <td>[@gastromom, the, only, pt, of, mine, who, ever, died, was, one, age, 21, profound, autism, chronic, underwt, &amp;, amp, ;, gi, issues, ,, given, lots, of, cipro, ., terrible]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, B-ADR, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tendon damage</td>\n",
              "      <td>[owww, ., i, hurt, my, foot, ., &amp;, amp, ;, am, concerned, ., 1st, warning, on, cipro, is, tendon, damage, ., :-/, it, certainly, was, n't, a, rupture, ., i, 'm, sure, i, 'm, just, paranoid, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ADR, I-ADR, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67e8f3b9-80fd-4a49-9e5a-8c54a4f0c7a4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-67e8f3b9-80fd-4a49-9e5a-8c54a4f0c7a4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-67e8f3b9-80fd-4a49-9e5a-8c54a4f0c7a4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import json"
      ],
      "metadata": {
        "id": "vHl13nU4HjI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ast.literal_eval(cadec_data.bio_tags.iloc[0])"
      ],
      "metadata": {
        "id": "FliNe-_THmO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cadec_data.rename(columns={'text_tokens': 'tweet'})"
      ],
      "metadata": {
        "id": "Cs41ak75ESDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cadec_data.columns = ['Unnamed: 0', 'text', 'extraction', 'indexes', 'tweet',\n",
        "       'bio_tags']"
      ],
      "metadata": {
        "id": "MwisbkJEG13u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cadec_data.tweet = [twee.strip('][').split(', ') for twee in cadec_data.tweet]\n",
        "cadec_data.bio_tags = [ast.literal_eval(tag) for tag in cadec_data.bio_tags]"
      ],
      "metadata": {
        "id": "OE-gcm0SG-Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to keep track of labels and index mapping \n",
        "all_tags = ','.join(cadec_data.bio_tags.apply(lambda x: ','.join(x)))\n",
        "unique_tags = np.unique(all_tags.split(','))\n",
        "labels_to_ids = {k: v for v, k in enumerate(unique_tags)}\n",
        "ids_to_labels = {v: k for v, k in enumerate(unique_tags)}"
      ],
      "metadata": {
        "id": "91xf2cGTLSNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(ids_to_labels)\n",
        "pprint(labels_to_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-4iPCeIMsyD",
        "outputId": "40c09e31-9900-4cdd-a499-ba2d191b0ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'B-ADR', 1: 'I-ADR', 2: 'O'}\n",
            "{'B-ADR': 0, 'I-ADR': 1, 'O': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 1\n",
        "VALID_BATCH_SIZE = 1\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
      ],
      "metadata": {
        "id": "Nr66p6-ROKA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding = tokenizer(final_train.tweet.iloc[0],\n",
        "#                         is_split_into_words=True,\n",
        "#                         return_offsets_mapping=True, \n",
        "#                         padding='max_length', \n",
        "#                         truncation=True, \n",
        "#                         max_length=128)"
      ],
      "metadata": {
        "id": "FPcDQN88OMY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for key, value in encoding.items():\n",
        "#     print(key, value)"
      ],
      "metadata": {
        "id": "F0BxWjnROVZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for token, offset in zip(tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"]), encoding['offset_mapping']):\n",
        "#   print('{0:10} {1}'.format(token, offset))"
      ],
      "metadata": {
        "id": "TtHjY4r5PXde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        tweet = self.data.tweet[index]\n",
        "        # text = nlp(self.data.tweet[index].strip())\n",
        "        # tweet = [token for token in text]\n",
        "        bio_tags = self.data.bio_tags[index]\n",
        "        encoding = self.tokenizer(tweet,\n",
        "                                 is_split_into_words=True,\n",
        "                                 return_offsets_mapping=True, \n",
        "                                 padding='max_length', \n",
        "                                 truncation=True, \n",
        "                                 max_length=self.max_len)\n",
        "        \n",
        "        labels = [labels_to_ids[label] for label in bio_tags]\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "\n",
        "        i = 0\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "            if mapping[0] == 0 and mapping[1] == 0:\n",
        "                # print(0,0)\n",
        "                continue\n",
        "            if mapping[0] == 0 and mapping[1] != 0:\n",
        "                encoded_labels[idx] = labels[i]\n",
        "                # print('first match')\n",
        "                i += 1\n",
        "            else:\n",
        "                # print('next match')\n",
        "                encoded_labels[idx] = encoded_labels[idx-1]\n",
        "        \n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['labels'] = torch.as_tensor(encoded_labels)\n",
        "\n",
        "        return item\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "xBlfZIQYOXuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = dataset(cadec_data, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(final_valid, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "J88tnfKN3uMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1UKG6hJFdRO",
        "outputId": "aeeaf011-6cb4-4d3e-dccc-c5c8d430b0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 0]),\n",
              " 'input_ids': tensor([  102,   259,   439,  2101,   190,  8693,  2675,   422,  3056, 11688,\n",
              "           137, 19756,  2754, 30113,   137,   259,   439,   692,  1795,   239,\n",
              "         17027, 10233,  1506,   205, 22007,   205, 26085, 11607,   603,   259,\n",
              "          4654,   198,   259,   360,  3645, 22783,  1796, 15832,  4554,   579,\n",
              "          1536,  3107, 30135,   234,  6569,   234, 16217,   256,   165,   165,\n",
              "          6571,   205,   563,   425,   482,  6569, 30113,   111, 26740, 13620,\n",
              "           107,   234,  2372,   247,  6005,   107,   121,  7858,   205,   259,\n",
              "          2505,   127,  3486,  4522,   282,   168,   106,   741,   121,  5060,\n",
              "           205,   198,   650,   572,   198,   205,   259,   572,   146,  2505,\n",
              "           105,   871,   582,   147,  3216,   137,  2744, 12651,   114,   137,\n",
              "         12651,   114,   205,   439,   575,  1942,   173,  1536,  2957,   650,\n",
              "           796,   205,   111,  1698, 10650,  8104,   327, 26046,  1922,   422,\n",
              "           563,  7345,  1121,  5296,  1994,   205,   103,     0]),\n",
              " 'labels': tensor([-100,    2,    2,    2,    2,    0,    1,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
              "            2,    2,    2,    2,    2,    2, -100, -100]),\n",
              " 'offset_mapping': tensor([[ 0,  0],\n",
              "         [ 0,  1],\n",
              "         [ 0,  2],\n",
              "         [ 0,  4],\n",
              "         [ 0,  4],\n",
              "         [ 0,  5],\n",
              "         [ 0,  4],\n",
              "         [ 0,  1],\n",
              "         [ 0,  6],\n",
              "         [ 0,  8],\n",
              "         [ 0,  3],\n",
              "         [ 0,  3],\n",
              "         [ 3,  5],\n",
              "         [ 5,  6],\n",
              "         [ 0,  3],\n",
              "         [ 0,  1],\n",
              "         [ 0,  2],\n",
              "         [ 0,  2],\n",
              "         [ 0,  5],\n",
              "         [ 0,  1],\n",
              "         [ 0,  3],\n",
              "         [ 0,  8],\n",
              "         [ 0,  5],\n",
              "         [ 0,  1],\n",
              "         [ 0,  3],\n",
              "         [ 0,  1],\n",
              "         [ 0,  3],\n",
              "         [ 3,  5],\n",
              "         [ 0,  4],\n",
              "         [ 0,  1],\n",
              "         [ 0,  3],\n",
              "         [ 0,  4],\n",
              "         [ 0,  1],\n",
              "         [ 0,  4],\n",
              "         [ 0,  2],\n",
              "         [ 2,  5],\n",
              "         [ 5,  8],\n",
              "         [ 8, 11],\n",
              "         [11, 14],\n",
              "         [ 0,  1],\n",
              "         [ 0,  2],\n",
              "         [ 0,  2],\n",
              "         [ 2,  3],\n",
              "         [ 0,  2],\n",
              "         [ 0,  2],\n",
              "         [ 0,  2],\n",
              "         [ 0,  8],\n",
              "         [ 0,  2],\n",
              "         [ 0,  2],\n",
              "         [ 0,  2],\n",
              "         [ 0,  4],\n",
              "         [ 0,  1],\n",
              "         [ 0,  3],\n",
              "         [ 0,  2],\n",
              "         [ 0,  3],\n",
              "         [ 0,  2],\n",
              "         [ 2,  3],\n",
              "         [ 0,  3],\n",
              "         [ 0,  3],\n",
              "         [ 3,  7],\n",
              "         [ 7,  9],\n",
              "         [ 0,  2],\n",
              "         [ 0,  3],\n",
              "         [ 3,  5],\n",
              "         [ 5,  8],\n",
              "         [ 8, 10],\n",
              "         [ 0,  2],\n",
              "         [ 0,  5],\n",
              "         [ 0,  1],\n",
              "         [ 0,  1],\n",
              "         [ 0,  1],\n",
              "         [ 1,  2],\n",
              "         [ 0,  4],\n",
              "         [ 4,  7],\n",
              "         [ 7,  9],\n",
              "         [ 0,  3],\n",
              "         [ 0,  1],\n",
              "         [ 0,  2],\n",
              "         [ 0,  2],\n",
              "         [ 0,  2],\n",
              "         [ 0,  1],\n",
              "         [ 0,  4],\n",
              "         [ 0,  4],\n",
              "         [ 0,  2],\n",
              "         [ 0,  4],\n",
              "         [ 0,  1],\n",
              "         [ 0,  1],\n",
              "         [ 0,  2],\n",
              "         [ 0,  1],\n",
              "         [ 1,  2],\n",
              "         [ 2,  3],\n",
              "         [ 0,  4],\n",
              "         [ 0,  5],\n",
              "         [ 0,  2],\n",
              "         [ 0,  4],\n",
              "         [ 0,  3],\n",
              "         [ 0,  3],\n",
              "         [ 0,  4],\n",
              "         [ 4,  6],\n",
              "         [ 0,  3],\n",
              "         [ 0,  4],\n",
              "         [ 4,  6],\n",
              "         [ 0,  1],\n",
              "         [ 0,  2],\n",
              "         [ 0,  2],\n",
              "         [ 2,  4],\n",
              "         [ 4,  6],\n",
              "         [ 0,  2],\n",
              "         [ 0,  5],\n",
              "         [ 0,  4],\n",
              "         [ 0,  2],\n",
              "         [ 0,  1],\n",
              "         [ 0,  3],\n",
              "         [ 0,  4],\n",
              "         [ 0,  4],\n",
              "         [ 0,  4],\n",
              "         [ 4,  7],\n",
              "         [ 7, 11],\n",
              "         [ 0,  4],\n",
              "         [ 0,  1],\n",
              "         [ 0,  3],\n",
              "         [ 0,  2],\n",
              "         [ 2,  5],\n",
              "         [ 0,  4],\n",
              "         [ 0,  4],\n",
              "         [ 0,  1],\n",
              "         [ 0,  0],\n",
              "         [ 0,  0]]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[1][\"input_ids\"]), training_set[1][\"labels\"]):\n",
        "  print('{0:10}  {1}'.format(token, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JKfkOFgPBe9",
        "outputId": "858db934-61fe-4d30-a793-1efb7071a90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]       -100\n",
            "i           2\n",
            "am          2\n",
            "left        2\n",
            "with        2\n",
            "chest       2\n",
            "pain        2\n",
            ",           2\n",
            "muscle      0\n",
            "weakness    1\n",
            "and         2\n",
            "spa         2\n",
            "##sm        2\n",
            "##s         2\n",
            "and         2\n",
            "i           2\n",
            "am          2\n",
            "up          2\n",
            "every       2\n",
            "3           2\n",
            "hrs         2\n",
            "drinking    2\n",
            "water       2\n",
            ".           2\n",
            "drs         2\n",
            ".           2\n",
            "lau         2\n",
            "##gh        2\n",
            "when        2\n",
            "i           2\n",
            "say         2\n",
            "that        2\n",
            "i           2\n",
            "have        2\n",
            "rh          2\n",
            "##abd       2\n",
            "##omy       2\n",
            "##olo       2\n",
            "##sis       2\n",
            "-           2\n",
            "my          2\n",
            "cp          2\n",
            "##k         2\n",
            "or          2\n",
            "ck          2\n",
            "or          2\n",
            "whatever    2\n",
            "it          2\n",
            "is          2\n",
            "is          2\n",
            "fine        2\n",
            ".           2\n",
            "but         2\n",
            "no          2\n",
            "one         2\n",
            "ck          2\n",
            "##s         2\n",
            "the         2\n",
            "myo         2\n",
            "##glob      2\n",
            "##in        2\n",
            "or          2\n",
            "hem         2\n",
            "##og        2\n",
            "##log       2\n",
            "##in        2\n",
            "in          2\n",
            "urine       2\n",
            ".           2\n",
            "i           2\n",
            "'           2\n",
            "m           2\n",
            "desp        2\n",
            "##ara       2\n",
            "##te        2\n",
            "for         2\n",
            "a           2\n",
            "dr          2\n",
            "in          2\n",
            "mn          2\n",
            ".           2\n",
            "that        2\n",
            "will        2\n",
            "do          2\n",
            "that        2\n",
            ".           2\n",
            "i           2\n",
            "do          2\n",
            "n           2\n",
            "'           2\n",
            "t           2\n",
            "know        2\n",
            "where       2\n",
            "to          2\n",
            "turn        2\n",
            "and         2\n",
            "get         2\n",
            "sick        2\n",
            "##er        2\n",
            "and         2\n",
            "sick        2\n",
            "##er        2\n",
            ".           2\n",
            "am          2\n",
            "af          2\n",
            "##ra        2\n",
            "##id        2\n",
            "my          2\n",
            "heart       2\n",
            "will        2\n",
            "go          2\n",
            ".           2\n",
            "the         2\n",
            "drug        2\n",
            "gets        2\n",
            "chol        2\n",
            "##est       2\n",
            "##orel      2\n",
            "down        2\n",
            ",           2\n",
            "but         2\n",
            "ru          2\n",
            "##ins       2\n",
            "your        2\n",
            "life        2\n",
            ".           2\n",
            "[SEP]       -100\n",
            "[PAD]       -100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "SQtu10EIouGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "QxlDItUXo4B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification"
      ],
      "metadata": {
        "id": "Axjdu1SUPyM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained('allenai/scibert_scivocab_uncased', num_labels=len(labels_to_ids)).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbD-1zUzoxiF",
        "outputId": "84f475c4-09b1-4b6a-d1c4-93f51039b9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameter(model):\n",
        "    return sum(para.numel() for para in model.parameters() if para.requires_grad)"
      ],
      "metadata": {
        "id": "jeHcLEHwhrJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The model has {count_parameter(model):,} trainable parameters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwWE1lC8h-cm",
        "outputId": "f4c48d28-ea8b-428d-ed2e-9e4da92b2273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 109,330,179 trainable parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = training_set[2]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"labels\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "outputs.loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpGyOmG_rajd",
        "outputId": "8ce4e0fd-12be-41be-829b-c2865d996395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0428, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "BEWTb3oiQbFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
        "def train(epoch, clip=1.0):\n",
        "    tr_loss, tr_f1 = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        # print(idx)\n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        labels = batch['labels'].to(device, dtype = torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        md = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        \n",
        "        md = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "        loss, tr_logits = md.loss, md.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        \n",
        "        # only compute accuracy at active labels\n",
        "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "        # torch.argmax(outputs.logits, axis=2)\n",
        "\n",
        "        mask_labels = labels.view(-1)\n",
        "        mask_predictions = torch.argmax(tr_logits, axis=2).view(-1)\n",
        "        # mask_predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        labels, predictions = list(), list()\n",
        "        for label in mask_labels.tolist():\n",
        "          labels.append(ids_to_labels.get(label, 'O'))\n",
        "        \n",
        "        for pred in mask_predictions.tolist():\n",
        "          predictions.append(ids_to_labels.get(pred, 'O'))\n",
        "\n",
        "        tr_labels.append(labels)\n",
        "        tr_preds.append(predictions)\n",
        "\n",
        "        tmp_tr_f1 = f1_score([labels], [predictions], average='micro')\n",
        "        tr_f1 += tmp_tr_f1\n",
        "    \n",
        "        loss.backward()\n",
        "\n",
        "        # gradient clipping\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_f1 = tr_f1 / nb_tr_steps\n",
        "    print(f\"Training loss epoch {epoch+1}: {epoch_loss}\")\n",
        "    print(f\"Training F1 score epoch {epoch+1}: {tr_f1}\\n\")"
      ],
      "metadata": {
        "id": "WbMZFL5Qo1B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAJzuntlpQ6f",
        "outputId": "89c6bbfa-2b39-4164-968f-3ecd7fb2c50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 1.1021047830581665\n",
            "Training loss per 100 training steps: 0.1779886517710615\n",
            "Training loss per 100 training steps: 0.17870816488782712\n",
            "Training loss per 100 training steps: 0.17046586343356443\n",
            "Training loss per 100 training steps: 0.17679200373692183\n",
            "Training loss per 100 training steps: 0.176638456174877\n",
            "Training loss per 100 training steps: 0.17326657735384665\n",
            "Training loss per 100 training steps: 0.17402756236893485\n",
            "Training loss per 100 training steps: 0.171179493458723\n",
            "Training loss per 100 training steps: 0.16613836916908828\n",
            "Training loss per 100 training steps: 0.16354341809284295\n",
            "Training loss per 100 training steps: 0.16059093851343195\n",
            "Training loss per 100 training steps: 0.15761305957658062\n",
            "Training loss per 100 training steps: 0.15709186247141915\n",
            "Training loss per 100 training steps: 0.15752536942201834\n",
            "Training loss per 100 training steps: 0.15511023948545494\n",
            "Training loss per 100 training steps: 0.1550513474044706\n",
            "Training loss per 100 training steps: 0.15493538237405696\n",
            "Training loss per 100 training steps: 0.1550349898302544\n",
            "Training loss per 100 training steps: 0.15332139689028987\n",
            "Training loss per 100 training steps: 0.152826498351132\n",
            "Training loss per 100 training steps: 0.15222511874094716\n",
            "Training loss per 100 training steps: 0.15129968915052155\n",
            "Training loss per 100 training steps: 0.15157484867802698\n",
            "Training loss per 100 training steps: 0.15242201399831168\n",
            "Training loss per 100 training steps: 0.15241819605321028\n",
            "Training loss per 100 training steps: 0.15208757804520143\n",
            "Training loss per 100 training steps: 0.15147664103149625\n",
            "Training loss per 100 training steps: 0.15087607069266346\n",
            "Training loss per 100 training steps: 0.15074578354248963\n",
            "Training loss per 100 training steps: 0.1500238481780945\n",
            "Training loss per 100 training steps: 0.1494743784710357\n",
            "Training loss per 100 training steps: 0.14910235444188696\n",
            "Training loss per 100 training steps: 0.14964463336197012\n",
            "Training loss per 100 training steps: 0.15009481004056333\n",
            "Training loss per 100 training steps: 0.1495927431324648\n",
            "Training loss per 100 training steps: 0.14980881168504842\n",
            "Training loss per 100 training steps: 0.14978998237383476\n",
            "Training loss per 100 training steps: 0.14933030904854838\n",
            "Training loss per 100 training steps: 0.14906450856093367\n",
            "Training loss per 100 training steps: 0.14869277000151876\n",
            "Training loss per 100 training steps: 0.1495770274534194\n",
            "Training loss per 100 training steps: 0.14887555521758575\n",
            "Training loss per 100 training steps: 0.14823874773108373\n",
            "Training loss per 100 training steps: 0.148519546089743\n",
            "Training loss per 100 training steps: 0.1485015642126247\n",
            "Training loss per 100 training steps: 0.14790817896478847\n",
            "Training loss per 100 training steps: 0.1474929820559265\n",
            "Training loss per 100 training steps: 0.14757752607251043\n",
            "Training loss per 100 training steps: 0.14753724253866415\n",
            "Training loss per 100 training steps: 0.1480548841965088\n",
            "Training loss per 100 training steps: 0.14796549379799118\n",
            "Training loss per 100 training steps: 0.14750500787892037\n",
            "Training loss per 100 training steps: 0.14705176788334445\n",
            "Training loss per 100 training steps: 0.14705747230051036\n",
            "Training loss per 100 training steps: 0.1473488045039566\n",
            "Training loss per 100 training steps: 0.1472650890473783\n",
            "Training loss per 100 training steps: 0.14720077236729268\n",
            "Training loss per 100 training steps: 0.1472963972592001\n",
            "Training loss per 100 training steps: 0.14716358623873627\n",
            "Training loss per 100 training steps: 0.14748004422547292\n",
            "Training loss per 100 training steps: 0.1478329780295939\n",
            "Training loss per 100 training steps: 0.1479670069394223\n",
            "Training loss per 100 training steps: 0.14796002528790123\n",
            "Training loss epoch 1: 0.1478608921506853\n",
            "Training F1 score epoch 1: 0.004506064995274235\n",
            "\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 0.04317634180188179\n",
            "Training loss per 100 training steps: 0.13664029583814416\n",
            "Training loss per 100 training steps: 0.1310615696462418\n",
            "Training loss per 100 training steps: 0.12517036035644552\n",
            "Training loss per 100 training steps: 0.12612595645585412\n",
            "Training loss per 100 training steps: 0.1271843030275461\n",
            "Training loss per 100 training steps: 0.12562198378068212\n",
            "Training loss per 100 training steps: 0.127470480798931\n",
            "Training loss per 100 training steps: 0.12989796045469992\n",
            "Training loss per 100 training steps: 0.1293915006079888\n",
            "Training loss per 100 training steps: 0.12900289287790656\n",
            "Training loss per 100 training steps: 0.13187477502931286\n",
            "Training loss per 100 training steps: 0.13278573708179145\n",
            "Training loss per 100 training steps: 0.13114169230548625\n",
            "Training loss per 100 training steps: 0.13419820077924707\n",
            "Training loss per 100 training steps: 0.13373560445081645\n",
            "Training loss per 100 training steps: 0.13237811354478166\n",
            "Training loss per 100 training steps: 0.1321278290217003\n",
            "Training loss per 100 training steps: 0.1306579408141733\n",
            "Training loss per 100 training steps: 0.13048509108578724\n",
            "Training loss per 100 training steps: 0.13139009298961507\n",
            "Training loss per 100 training steps: 0.13236820917516265\n",
            "Training loss per 100 training steps: 0.13215807499935253\n",
            "Training loss per 100 training steps: 0.1319835449438194\n",
            "Training loss per 100 training steps: 0.13226542369761612\n",
            "Training loss per 100 training steps: 0.13223306513594382\n",
            "Training loss per 100 training steps: 0.13170664559516973\n",
            "Training loss per 100 training steps: 0.13257945319123896\n",
            "Training loss per 100 training steps: 0.13209630019652902\n",
            "Training loss per 100 training steps: 0.13206074590884953\n",
            "Training loss per 100 training steps: 0.13278994490015447\n",
            "Training loss per 100 training steps: 0.13364504421038523\n",
            "Training loss per 100 training steps: 0.1338888306558535\n",
            "Training loss per 100 training steps: 0.13361988706379943\n",
            "Training loss per 100 training steps: 0.1333311478099661\n",
            "Training loss per 100 training steps: 0.13305433822707433\n",
            "Training loss per 100 training steps: 0.13349475378431308\n",
            "Training loss per 100 training steps: 0.13333535510998837\n",
            "Training loss per 100 training steps: 0.1339674318238448\n",
            "Training loss per 100 training steps: 0.13412824576845694\n",
            "Training loss per 100 training steps: 0.13385863393492545\n",
            "Training loss per 100 training steps: 0.1339457398840294\n",
            "Training loss per 100 training steps: 0.13369511418947735\n",
            "Training loss per 100 training steps: 0.13346708273810462\n",
            "Training loss per 100 training steps: 0.1337771496631398\n",
            "Training loss per 100 training steps: 0.1345117917394862\n",
            "Training loss per 100 training steps: 0.1345362974303659\n",
            "Training loss per 100 training steps: 0.1353035503762646\n",
            "Training loss per 100 training steps: 0.1350764558572765\n",
            "Training loss per 100 training steps: 0.13505501528699054\n",
            "Training loss per 100 training steps: 0.13491734031474614\n",
            "Training loss per 100 training steps: 0.13448634175466873\n",
            "Training loss per 100 training steps: 0.1347633625028472\n",
            "Training loss per 100 training steps: 0.13458410089238537\n",
            "Training loss per 100 training steps: 0.13512763689663362\n",
            "Training loss per 100 training steps: 0.1350494340920432\n",
            "Training loss per 100 training steps: 0.13537111108664496\n",
            "Training loss per 100 training steps: 0.13539831223651572\n",
            "Training loss per 100 training steps: 0.13522054806685074\n",
            "Training loss per 100 training steps: 0.13557211903252434\n",
            "Training loss per 100 training steps: 0.1357420686462127\n",
            "Training loss per 100 training steps: 0.13588916388279196\n",
            "Training loss per 100 training steps: 0.13595774964168073\n",
            "Training loss per 100 training steps: 0.13616004012788877\n",
            "Training loss epoch 2: 0.13649681424208668\n",
            "Training F1 score epoch 2: 0.009375080705152437\n",
            "\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 0.06345941871404648\n",
            "Training loss per 100 training steps: 0.12816966325044632\n",
            "Training loss per 100 training steps: 0.12709950195597625\n",
            "Training loss per 100 training steps: 0.12019198593618564\n",
            "Training loss per 100 training steps: 0.1228497937261278\n",
            "Training loss per 100 training steps: 0.12271353145949766\n",
            "Training loss per 100 training steps: 0.12255155795738845\n",
            "Training loss per 100 training steps: 0.12334455667759761\n",
            "Training loss per 100 training steps: 0.12304985078142378\n",
            "Training loss per 100 training steps: 0.12345602657991181\n",
            "Training loss per 100 training steps: 0.12532917151160605\n",
            "Training loss per 100 training steps: 0.1255767341074259\n",
            "Training loss per 100 training steps: 0.12637343562691225\n",
            "Training loss per 100 training steps: 0.12668743528687695\n",
            "Training loss per 100 training steps: 0.129334402921577\n",
            "Training loss per 100 training steps: 0.13015927239519723\n",
            "Training loss per 100 training steps: 0.128591174651711\n",
            "Training loss per 100 training steps: 0.12887619404099784\n",
            "Training loss per 100 training steps: 0.12833895714219756\n",
            "Training loss per 100 training steps: 0.12969319851197467\n",
            "Training loss per 100 training steps: 0.13046096236680516\n",
            "Training loss per 100 training steps: 0.13197346064196708\n",
            "Training loss per 100 training steps: 0.13150331531999349\n",
            "Training loss per 100 training steps: 0.13194371182353126\n",
            "Training loss per 100 training steps: 0.13223876858180328\n",
            "Training loss per 100 training steps: 0.1323313955466629\n",
            "Training loss per 100 training steps: 0.13212982990628774\n",
            "Training loss per 100 training steps: 0.13201436509885292\n",
            "Training loss per 100 training steps: 0.13158648482674146\n",
            "Training loss per 100 training steps: 0.1314050529690049\n",
            "Training loss per 100 training steps: 0.13198528543879023\n",
            "Training loss per 100 training steps: 0.13139334318071294\n",
            "Training loss per 100 training steps: 0.13208600445880367\n",
            "Training loss per 100 training steps: 0.13173312219871242\n",
            "Training loss per 100 training steps: 0.1317146446602953\n",
            "Training loss per 100 training steps: 0.13154512287684952\n",
            "Training loss per 100 training steps: 0.13161591377818707\n",
            "Training loss per 100 training steps: 0.1321234790760152\n",
            "Training loss per 100 training steps: 0.132558821519137\n",
            "Training loss per 100 training steps: 0.1325222902222336\n",
            "Training loss per 100 training steps: 0.13271024899450856\n",
            "Training loss per 100 training steps: 0.13232315691900834\n",
            "Training loss per 100 training steps: 0.13273506185900702\n",
            "Training loss per 100 training steps: 0.1324548061723914\n",
            "Training loss per 100 training steps: 0.13219222871057004\n",
            "Training loss per 100 training steps: 0.13234358389263484\n",
            "Training loss per 100 training steps: 0.13200566556195995\n",
            "Training loss per 100 training steps: 0.13191096965988064\n",
            "Training loss per 100 training steps: 0.13159572906250752\n",
            "Training loss per 100 training steps: 0.13194910003759014\n",
            "Training loss per 100 training steps: 0.13193640727787698\n",
            "Training loss per 100 training steps: 0.13197665869432157\n",
            "Training loss per 100 training steps: 0.13196505383569618\n",
            "Training loss per 100 training steps: 0.13224349593568388\n",
            "Training loss per 100 training steps: 0.1321583272235145\n",
            "Training loss per 100 training steps: 0.13248351846606468\n",
            "Training loss per 100 training steps: 0.13280757526766535\n",
            "Training loss per 100 training steps: 0.13283750093926142\n",
            "Training loss per 100 training steps: 0.13290961627651818\n",
            "Training loss per 100 training steps: 0.13247785005666426\n",
            "Training loss per 100 training steps: 0.13255957354453293\n",
            "Training loss per 100 training steps: 0.13254779001799868\n",
            "Training loss per 100 training steps: 0.13235740741976765\n",
            "Training loss per 100 training steps: 0.13204363786348347\n",
            "Training loss epoch 3: 0.1325207268895164\n",
            "Training F1 score epoch 3: 0.011589183151712753\n",
            "\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 0.23129846155643463\n",
            "Training loss per 100 training steps: 0.14724124294687246\n",
            "Training loss per 100 training steps: 0.14189105653157688\n",
            "Training loss per 100 training steps: 0.14571123545218964\n",
            "Training loss per 100 training steps: 0.14304428546285336\n",
            "Training loss per 100 training steps: 0.13920870334692598\n",
            "Training loss per 100 training steps: 0.13743839542824657\n",
            "Training loss per 100 training steps: 0.13401191199290782\n",
            "Training loss per 100 training steps: 0.13309721867108068\n",
            "Training loss per 100 training steps: 0.1342963475778604\n",
            "Training loss per 100 training steps: 0.13209696595360954\n",
            "Training loss per 100 training steps: 0.128839706208548\n",
            "Training loss per 100 training steps: 0.12780101813229972\n",
            "Training loss per 100 training steps: 0.12891138822140408\n",
            "Training loss per 100 training steps: 0.12870366171581224\n",
            "Training loss per 100 training steps: 0.12954931939932066\n",
            "Training loss per 100 training steps: 0.12887703956681149\n",
            "Training loss per 100 training steps: 0.1288726491177149\n",
            "Training loss per 100 training steps: 0.12813040556728847\n",
            "Training loss per 100 training steps: 0.12635791043789046\n",
            "Training loss per 100 training steps: 0.12621784110673542\n",
            "Training loss per 100 training steps: 0.125571160282241\n",
            "Training loss per 100 training steps: 0.12657954076581246\n",
            "Training loss per 100 training steps: 0.1260673717892348\n",
            "Training loss per 100 training steps: 0.1259543004301079\n",
            "Training loss per 100 training steps: 0.1257851952405788\n",
            "Training loss per 100 training steps: 0.12670040060202373\n",
            "Training loss per 100 training steps: 0.12743566696670608\n",
            "Training loss per 100 training steps: 0.1272670986644234\n",
            "Training loss per 100 training steps: 0.1271549227038505\n",
            "Training loss per 100 training steps: 0.12696113463234485\n",
            "Training loss per 100 training steps: 0.1268537478398054\n",
            "Training loss per 100 training steps: 0.12805043436821006\n",
            "Training loss per 100 training steps: 0.12740992507670773\n",
            "Training loss per 100 training steps: 0.12765600207913758\n",
            "Training loss per 100 training steps: 0.12783278395121478\n",
            "Training loss per 100 training steps: 0.12823948991368825\n",
            "Training loss per 100 training steps: 0.128796256411251\n",
            "Training loss per 100 training steps: 0.1291298378483677\n",
            "Training loss per 100 training steps: 0.12947454210199372\n",
            "Training loss per 100 training steps: 0.1291428930635484\n",
            "Training loss per 100 training steps: 0.12917087456889564\n",
            "Training loss per 100 training steps: 0.1284036395583862\n",
            "Training loss per 100 training steps: 0.12815170369159729\n",
            "Training loss per 100 training steps: 0.12784606484388586\n",
            "Training loss per 100 training steps: 0.12781849303840853\n",
            "Training loss per 100 training steps: 0.12740935649347943\n",
            "Training loss per 100 training steps: 0.1276295898610422\n",
            "Training loss per 100 training steps: 0.12728821341527977\n",
            "Training loss per 100 training steps: 0.12821396415541628\n",
            "Training loss per 100 training steps: 0.12793288219581206\n",
            "Training loss per 100 training steps: 0.12761742843688198\n",
            "Training loss per 100 training steps: 0.1273053764379102\n",
            "Training loss per 100 training steps: 0.12727302581937167\n",
            "Training loss per 100 training steps: 0.127412698149398\n",
            "Training loss per 100 training steps: 0.12810440051697816\n",
            "Training loss per 100 training steps: 0.1283618297434588\n",
            "Training loss per 100 training steps: 0.128045286463325\n",
            "Training loss per 100 training steps: 0.12801720204836006\n",
            "Training loss per 100 training steps: 0.12837465505839762\n",
            "Training loss per 100 training steps: 0.12825344676721354\n",
            "Training loss per 100 training steps: 0.1286686844903271\n",
            "Training loss per 100 training steps: 0.1284297477410324\n",
            "Training loss per 100 training steps: 0.12859133436420606\n",
            "Training loss epoch 4: 0.12875640614922076\n",
            "Training F1 score epoch 4: 0.010573586396685809\n",
            "\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 0.22386017441749573\n",
            "Training loss per 100 training steps: 0.09728550750965087\n",
            "Training loss per 100 training steps: 0.11134857566510582\n",
            "Training loss per 100 training steps: 0.11931616045956955\n",
            "Training loss per 100 training steps: 0.11689508172185241\n",
            "Training loss per 100 training steps: 0.11696335572601592\n",
            "Training loss per 100 training steps: 0.11833036760702735\n",
            "Training loss per 100 training steps: 0.11953590616544632\n",
            "Training loss per 100 training steps: 0.12225830048213965\n",
            "Training loss per 100 training steps: 0.12350425052045164\n",
            "Training loss per 100 training steps: 0.12516148299294907\n",
            "Training loss per 100 training steps: 0.12331361970863179\n",
            "Training loss per 100 training steps: 0.1238469865277691\n",
            "Training loss per 100 training steps: 0.12408612666415912\n",
            "Training loss per 100 training steps: 0.12317586702819888\n",
            "Training loss per 100 training steps: 0.123893798537743\n",
            "Training loss per 100 training steps: 0.12455192168648174\n",
            "Training loss per 100 training steps: 0.12605370837457103\n",
            "Training loss per 100 training steps: 0.12701968412918657\n",
            "Training loss per 100 training steps: 0.12645197924381335\n",
            "Training loss per 100 training steps: 0.1272535360281424\n",
            "Training loss per 100 training steps: 0.1260048451159426\n",
            "Training loss per 100 training steps: 0.12572349508898903\n",
            "Training loss per 100 training steps: 0.12524116318662776\n",
            "Training loss per 100 training steps: 0.1253592768007017\n",
            "Training loss per 100 training steps: 0.1257174773173999\n",
            "Training loss per 100 training steps: 0.12537602278571242\n",
            "Training loss per 100 training steps: 0.1247351668806847\n",
            "Training loss per 100 training steps: 0.12464953104283188\n",
            "Training loss per 100 training steps: 0.1249520483006234\n",
            "Training loss per 100 training steps: 0.12502981848020206\n",
            "Training loss per 100 training steps: 0.12493615645128181\n",
            "Training loss per 100 training steps: 0.12568882654397537\n",
            "Training loss per 100 training steps: 0.1251076863676354\n",
            "Training loss per 100 training steps: 0.12435741947177029\n",
            "Training loss per 100 training steps: 0.12413825415640044\n",
            "Training loss per 100 training steps: 0.12432002769153863\n",
            "Training loss per 100 training steps: 0.12450136510986726\n",
            "Training loss per 100 training steps: 0.12398427685464243\n",
            "Training loss per 100 training steps: 0.12490904885322274\n",
            "Training loss per 100 training steps: 0.1253100727632262\n",
            "Training loss per 100 training steps: 0.12547213346086075\n",
            "Training loss per 100 training steps: 0.1257938697492612\n",
            "Training loss per 100 training steps: 0.12554870528357823\n",
            "Training loss per 100 training steps: 0.12571270499693143\n",
            "Training loss per 100 training steps: 0.1258546792454354\n",
            "Training loss per 100 training steps: 0.1257189026630189\n",
            "Training loss per 100 training steps: 0.12552132454433293\n",
            "Training loss per 100 training steps: 0.12600901259634567\n",
            "Training loss per 100 training steps: 0.12598634473922346\n",
            "Training loss per 100 training steps: 0.12672664183099655\n",
            "Training loss per 100 training steps: 0.12697139869450713\n",
            "Training loss per 100 training steps: 0.12737743739681262\n",
            "Training loss per 100 training steps: 0.12738592142805347\n",
            "Training loss per 100 training steps: 0.12720415062873117\n",
            "Training loss per 100 training steps: 0.1270781497307363\n",
            "Training loss per 100 training steps: 0.1271060624288447\n",
            "Training loss per 100 training steps: 0.12717208821043308\n",
            "Training loss per 100 training steps: 0.12731141982187955\n",
            "Training loss per 100 training steps: 0.12741892169538688\n",
            "Training loss per 100 training steps: 0.12737079117599676\n",
            "Training loss per 100 training steps: 0.12723427145929012\n",
            "Training loss per 100 training steps: 0.12744073283272503\n",
            "Training loss per 100 training steps: 0.12714336034516754\n",
            "Training loss epoch 5: 0.12723607607696943\n",
            "Training F1 score epoch 5: 0.013062391269031451\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['labels'].to(device, dtype = torch.long)\n",
        "            \n",
        "            md = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            loss, eval_logits = md.loss, md.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "\n",
        "            mask_labels = labels.view(-1)\n",
        "            mask_predictions = torch.argmax(eval_logits, axis=2).view(-1)\n",
        "\n",
        "            # mask_labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            # mask_predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            labels, predictions = list(), list()\n",
        "            for label in mask_labels.tolist():\n",
        "            #   labels.append(ids_to_labels[label])\n",
        "              labels.append(ids_to_labels.get(label, 'O'))\n",
        "            \n",
        "            for pred in mask_predictions.tolist():\n",
        "            #   predictions.append(ids_to_labels[pred])\n",
        "              predictions.append(ids_to_labels.get(pred, 'O'))\n",
        "\n",
        "            eval_labels.append(labels)\n",
        "            eval_preds.append(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = f1_score([labels], [predictions], average='micro')\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    # labels = [[ids_to_labels[id.item()] for id in lab] for lab in eval_labels]\n",
        "    # predictions = [[ids_to_labels[id.item()] for id in lab] for lab in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"\\nValidation Loss: {eval_loss}\")\n",
        "    print(f\"Validation F1 score: {eval_accuracy}\")\n",
        "\n",
        "    return eval_labels, eval_preds"
      ],
      "metadata": {
        "id": "5YIpPJ8kpQ_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, predictions = valid(model, testing_loader)"
      ],
      "metadata": {
        "id": "Zv3v0fDiuANs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02874b0-b770-417b-d768-3f94d03beca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.7478603720664978\n",
            "Validation loss per 100 evaluation steps: 0.6151311755950314\n",
            "Validation loss per 100 evaluation steps: 0.5940618759922658\n",
            "Validation loss per 100 evaluation steps: 0.6275316970350346\n",
            "\n",
            "Validation Loss: 0.6286164516967502\n",
            "Validation F1 score: 0.03555705410170649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(labels, predictions))"
      ],
      "metadata": {
        "id": "iZsUFwNHTLa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ca9cbc-20c5-4b4f-9a59-de1aa08a2392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADR       0.03      0.09      0.04       481\n",
            "\n",
            "   micro avg       0.03      0.09      0.04       481\n",
            "   macro avg       0.03      0.09      0.04       481\n",
            "weighted avg       0.03      0.09      0.04       481\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.scheme import IOB2"
      ],
      "metadata": {
        "id": "ejBD06ZcTxVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(labels, predictions, mode='strict', scheme=IOB2))"
      ],
      "metadata": {
        "id": "0k5WiAUaThVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13eff91b-5624-4c5b-ef44-7bd08467857e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADR       0.04      0.08      0.05       481\n",
            "\n",
            "   micro avg       0.04      0.08      0.05       481\n",
            "   macro avg       0.04      0.08      0.05       481\n",
            "weighted avg       0.04      0.08      0.05       481\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JoalSx7YfL_t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}